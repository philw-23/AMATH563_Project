{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "flexible-occasions",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import zipfile\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from ModelStructures import select_model\n",
    "from math import floor, ceil\n",
    "\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "official-premium",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2363 305\n"
     ]
    }
   ],
   "source": [
    "# Read in Data\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "image_scenes = pd.read_csv('final_box_labels_grozi.csv')\n",
    "image_scenes['image_id'] = pd.factorize(image_scenes['image_path'])[0] # Encode file to integer\n",
    "\n",
    "# Create data sets\n",
    "scene_ids = image_scenes['image_id'].unique()\n",
    "scenes_copy = image_scenes.copy() # For filtering\n",
    "random.Random(1).shuffle(scene_ids)\n",
    "train_ids = scene_ids[:len(scene_ids)-80]\n",
    "train_scenes = scenes_copy[scenes_copy['image_id'].isin(train_ids)]\n",
    "val_scenes = scenes_copy[~scenes_copy['image_id'].isin(train_ids)]\n",
    "print(len(train_scenes.index), len(val_scenes.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dependent-intention",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Custom image dataset\n",
    "class DetectionImageDataset():\n",
    "    def __init__(self, image_frame, transforms):\n",
    "        self.image_frame = image_frame\n",
    "        self.transforms = transforms        \n",
    "        self.images = self.image_frame['image_path'].unique()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        img = Image.open(img_path) # Open the image\n",
    "        width, height = img.size # Get size of image\n",
    "        \n",
    "        image_labels = self.image_frame[self.image_frame['image_path'] == \\\n",
    "                                       img_path]\n",
    "        \n",
    "        # Get all the boxes and labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for idx, row in image_labels.iterrows():\n",
    "            x_min = row['bbox_x'] * width\n",
    "            x_max = row['bbox_w'] * width\n",
    "            y_min = row['bbox_y'] * height\n",
    "            y_max = row['bbox_h'] * height\n",
    "            boxes.append([x_min, y_min, x_max, y_max])\n",
    "            labels.append(row['label'])\n",
    "        \n",
    "        image_id = image_labels['image_id'].unique()\n",
    "        \n",
    "        # Convert everything to tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64) # Assume all instances are not crowd\n",
    "        image_id = torch.as_tensor(image_id)\n",
    "        areas = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        # Generate targets dictionary\n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = image_id\n",
    "        target['area'] = areas\n",
    "        target['iscrowd'] = iscrowd\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        \n",
    "        return img, target\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "heard-blade",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Import transforms helper\n",
    "import transforms as T\n",
    "import utils\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    \n",
    "    return T.Compose(transforms)\n",
    "\n",
    "# Datasets\n",
    "train_set = DetectionImageDataset(train_scenes, transforms=get_transform(True))\n",
    "val_set = DetectionImageDataset(val_scenes, transforms=get_transform(False))\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=2, shuffle=True,\n",
    "                                          pin_memory=True, num_workers=0,\n",
    "                                          collate_fn=utils.collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=1, shuffle=False,\n",
    "                                          pin_memory=True, num_workers=0,\n",
    "                                        collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "personal-coral",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# Unzip images\n",
    "if not os.path.exists('./Testing'):\n",
    "    test_zipped = zipfile.ZipFile('./Testing.zip', 'r')\n",
    "    test_zipped.extractall()\n",
    "    test_zipped.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "elder-grocery",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "Test:  [ 0/80]  eta: 0:00:57  loss: 1.4123 (1.4123)  model_time: 0.1058 (0.1058)  evaluator_time: 0.0072 (0.0072)  time: 0.7238  data: 0.4628  max mem: 770\n",
      "Test:  [79/80]  eta: 0:00:00  loss: 0.3858 (0.5282)  model_time: 0.0971 (0.0970)  evaluator_time: 0.0036 (0.0041)  time: 0.5462  data: 0.3338  max mem: 770\n",
      "Test: Total time: 0:00:44 (0.5501 s / it)\n",
      "Averaged stats: loss: 0.3858 (0.5282)  model_time: 0.0971 (0.0970)  evaluator_time: 0.0036 (0.0041)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.15s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.41248\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.64462\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.43571\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.00000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.00000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.41248\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.18236\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.57869\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.58312\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.00000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.00000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.58312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<coco_eval.CocoEvaluator at 0x7fe097f962e0>,\n",
       " <utils.MetricLogger at 0x7fe0c0027d00>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Model\n",
    "n_param_g = 80\n",
    "f_path = './Detection/Archive/ourweights=True_reqgrad=True_pretrain=True/plateau/'\n",
    "m_dict = torch.load(f_path + 'AMATH563_Grozi_Detection_Best.pth', map_location=device)\n",
    "loaded_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False,\n",
    "                                                                    progress=False,\n",
    "                                                                    pretrained_backbone=False)\n",
    "in_features = loaded_model.roi_heads.box_predictor.cls_score.in_features\n",
    "loaded_model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_param_g + 1)\n",
    "loaded_model.transform.max_size = 3264 # Update max size\n",
    "loaded_model.to(device)\n",
    "loaded_model.load_state_dict(m_dict)\n",
    "\n",
    "# Evaluation Metrics\n",
    "from engine import evaluate\n",
    "evaluate(loaded_model, val_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "criminal-connectivity",
   "metadata": {
    "gradient": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5115\n"
     ]
    }
   ],
   "source": [
    "# Generate Test Scenarios!\n",
    "if not os.path.exists('./TestingOutputs'):\n",
    "    os.mkdir('./TestingOutputs')\n",
    "    \n",
    "import time\n",
    "\n",
    "# Load the Grozi Mappings\n",
    "grozi_mappings = pd.read_csv('./our_encoded_grozi_mappings.csv')[['label', 'label_encoded']]\n",
    "grozi_mappings.drop_duplicates(inplace=True)\n",
    "grozi_dict = dict(zip(grozi_mappings['label_encoded'].astype(str), grozi_mappings['label']))\n",
    "\n",
    "font = ImageFont.truetype('AdobeVFPrototype.ttf', 36)\n",
    "\n",
    "# device = 'cpu'\n",
    "# loaded_model.to(device)\n",
    "loaded_model.eval()\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    for i, item in enumerate(val_set):\n",
    "        img, targets = val_set[i]\n",
    "        img = img.to(device)\n",
    "        prediction = loaded_model([img]) \n",
    "    end = time.time()\n",
    "    mean_inf_time = round((end - start) / len(val_set), 4)\n",
    "    print(mean_inf_time)\n",
    "#         label_boxes = targets['boxes']\n",
    "#         img = img.cpu()\n",
    "#         image = Image.fromarray(img.mul(255).permute(1, 2,0).byte().numpy())\n",
    "#         draw = ImageDraw.Draw(image)\n",
    "\n",
    "#         for elem in range(len(label_boxes)):\n",
    "#             draw.rectangle([(label_boxes[elem][0], label_boxes[elem][1]),\n",
    "#                             (label_boxes[elem][2], label_boxes[elem][3])], \n",
    "#             outline =\"green\", width =3)\n",
    "\n",
    "#         for element in range(len(prediction[0][\"boxes\"])):\n",
    "#             boxes = prediction[0][\"boxes\"][element].cpu().numpy()\n",
    "#             score = np.round(prediction[0][\"scores\"][element].cpu().numpy(),\n",
    "#                                     decimals= 4)\n",
    "#             label = prediction[0]['labels'][element].cpu().numpy()\n",
    "#             lab_print = grozi_dict[str(label)]\n",
    "\n",
    "#             if score > 0.5:\n",
    "#                 draw.rectangle([(boxes[0], boxes[1]), (boxes[2], boxes[3])], \n",
    "#                                 outline =\"red\", width =3)\n",
    "#                 label_text = str(score) + ', ' + lab_print\n",
    "#                 draw.text((boxes[0], boxes[1]), text = label_text, font=font)   \n",
    "                \n",
    "#         image.save('./TestingOutputs/test_instance_' + str(i) + '.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
